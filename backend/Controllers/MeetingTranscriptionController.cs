using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Authorization;
using Microsoft.EntityFrameworkCore;
using backend.Models;
using backend.Models.Entities;
using System.Text.Json;

namespace backend.Controllers;

/// <summary>
/// Controller f√∂r hantering av m√∂tesljudinspelningar och AI-transkribering.
/// 
/// Denna controller ansvarar f√∂r:
/// 1. Ta emot ljudfiler fr√•n frontend (antingen uppladdade filer eller live-inspelningar)
/// 2. Skicka ljudet till AI-tj√§nster f√∂r transkribering (f√∂r n√§rvarande simulerat)
/// 3. Spara transkriberingen i databasen f√∂r framtida √•tkomst
/// 4. Tillhandah√•lla endpoints f√∂r att h√§mta tidigare transkriberingar
/// 
/// S√§kerhet: Alla endpoints kr√§ver Azure AD-autentisering.
/// AI-integration: F√∂rberedd f√∂r OpenAI Whisper API men anv√§nder testdata f√∂r utveckling.
/// </summary>
[ApiController]
[Route("api/[controller]")]
[AllowAnonymous] // Temporary - remove auth requirement for testing
public class MeetingTranscriptionController : ControllerBase
{
    // Dependency injection f√∂r externa tj√§nster
    private readonly IHttpClientFactory _httpClientFactory; // F√∂r framtida OpenAI API-anrop
    private readonly IConfiguration _configuration;         // F√∂r att l√§sa API-nycklar
    private readonly ILogger<MeetingTranscriptionController> _logger; // Logging
    private readonly AppDbContext _context;                // Databas√•tkomst

    /// <summary>
    /// Constructor som injicerar alla n√∂dv√§ndiga beroenden.
    /// </summary>
    public MeetingTranscriptionController(
        IHttpClientFactory httpClientFactory,
        IConfiguration configuration,
        ILogger<MeetingTranscriptionController> logger,
        AppDbContext context)
    {
        _httpClientFactory = httpClientFactory;
        _configuration = configuration;
        _logger = logger;
        _context = context;
    }

    /// <summary>
    /// Huvudendpoint f√∂r att ladda upp ljudfiler och f√• AI-transkribering.
    /// 
    /// Denna metod:
    /// 1. Validerar den uppladdade filen (storlek, format, s√§kerhet)
    /// 2. Kontrollerar anv√§ndarens beh√∂righet via Azure AD-claims
    /// 3. Skickar filen f√∂r AI-transkribering (simulerat f√∂r utveckling)
    /// 4. Sparar resultatet i databasen f√∂r framtida √•tkomst
    /// 5. Returnerar transkriberingen till frontend
    /// 
    /// Parameter audioFile: Den uppladdade ljudfilen (.wav, .mp3, max 25MB)
    /// Parameter meetingId: ID f√∂r bokningen/m√∂tet som inspelningen tillh√∂r
    /// Parameter userId: Anv√§nds f√∂r extra validering (borde matcha Azure AD-claim)
    /// 
    /// Returns: JSON med transkribering, sammanfattning och √•tg√§rdspunkter
    /// </summary>
    [HttpPost("upload-and-transcribe")]
    public async Task<IActionResult> UploadAndTranscribe(IFormFile audioFile, [FromForm] string meetingId, [FromForm] string userId)
    {
        try
        {
            // === FILVALIDERING ===
            // Kontrollera att en fil faktiskt laddades upp
            if (audioFile == null || audioFile.Length == 0)
            {
                return BadRequest("Ingen ljudfil uppladdad");
            }

            // S√§kerhetsgr√§ns: F√∂rhindra f√∂r stora filer som kan √∂verbelasta servern
            if (audioFile.Length > 25 * 1024 * 1024) // 25MB limit
            {
                return BadRequest("Filen √§r f√∂r stor. Max 25MB till√•tet.");
            }

            // S√§kerhet: Endast till√•t k√§nda ljudformat (webm, wav, mp3) f√∂r att f√∂rhindra skadliga filer
            // Whisper st√∂djer these common container types; block everything else.
            var allowedTypes = new[] { "audio/webm", "audio/wav", "audio/mp3", "audio/mpeg" };
            if (!allowedTypes.Contains(audioFile.ContentType.ToLower()))
            {
                return BadRequest("Endast .wav och .mp3 filer √§r till√•tna");
            }

            // === S√ÑKERHETSVALIDERING ===
            // TEMP: Skip user validation since we're using AllowAnonymous for testing
            var currentUserId = userId; // Use the provided userId from form
            if (string.IsNullOrEmpty(currentUserId))
            {
                currentUserId = "12345678-1234-1234-1234-123456789012"; // Fallback GUID for testing
            }

            // === AI-TRANSKRIBERING ===
            // Om OpenAI-nyckel √§r konfigurerad, anv√§nd Whisper f√∂r transkribering och
            // anv√§nd √§ven Chat Completions f√∂r att skapa summary + action points.
            // Annars faller vi tillbaka till simulerad transkribering (utvecklingsl√§ge).
            var openAiKey = _configuration["OpenAI:ApiKey"];
            TranscriptionResult transcriptionResult;
            
            if (!string.IsNullOrEmpty(openAiKey))
            {
                _logger.LogInformation("üéôÔ∏è USING REAL OPENAI WHISPER - API key found! File: {FileName}", audioFile.FileName);
                transcriptionResult = await TranscribeWithOpenAIAsync(audioFile);
                _logger.LogInformation("‚úÖ OpenAI transcription completed successfully");
            }
            else
            {
                _logger.LogWarning("‚ö†Ô∏è NO OPENAI KEY - Using simulated transcription (mock data)");
                // Fallback f√∂r utveckling / tests
                transcriptionResult = await SimulateTranscription(audioFile);
                _logger.LogWarning("üìù Mock transcription completed - NOT REAL AI RESULT!");
            }

            // === DATABASSPARNING ===
            // TEMP: Use fallback username since no auth claims available
            var userName = "Test User"; // Fallback for anonymous testing
            
            // Konvertera meetingId till int och spara i databasen f√∂r framtida √•tkomst
            if (int.TryParse(meetingId, out int bookingId))
            {
                await SaveTranscriptionResult(bookingId, currentUserId, userName, audioFile, transcriptionResult);
            }

            // === SVAR TILL FRONTEND ===
            // Returnera det kompletta resultatet f√∂r omedelbar visning
            return Ok(new
            {
                success = true,
                message = "Transkribering slutf√∂rd",
                transcription = transcriptionResult.Transcription,
                summary = transcriptionResult.Summary,
                actionPoints = transcriptionResult.ActionPoints,
                timestamp = DateTime.UtcNow
            });
        }
        catch (Exception ex)
        {
            // Logga fel f√∂r fels√∂kning men exponera inte k√§nslig information till anv√§ndaren
            _logger.LogError(ex, "Fel vid transkribering av ljudfil");
            return StatusCode(500, new { success = false, message = $"Fel vid transkribering: {ex.Message}" });
        }
    }

    /// <summary>
    /// Simulerar AI-transkribering av ljudfil f√∂r utveckling och testning.
    /// 
    /// VIKTIGT: Detta √§r endast f√∂r utveckling!
    /// I produktion skulle denna metod:
    /// 1. Konvertera ljudfilen till r√§tt format f√∂r AI-API:et
    /// 2. Skicka HTTP-request till OpenAI Whisper API
    /// 3. Hantera API-svar och fel
    /// 4. Returnera verklig transkribering
    /// 
    /// F√∂r nu returnerar vi realistisk testdata f√∂r att demonstrera funktionaliteten.
    /// </summary>
    /// <param name="audioFile">Ljudfilen som ska transkriberas</param>
    /// <returns>Simulerat transkriberingsresultat med text, sammanfattning och √•tg√§rdspunkter</returns>
    private async Task<TranscriptionResult> SimulateTranscription(IFormFile audioFile)
    {
        // Simulera AI-bearbetningstid f√∂r realistisk anv√§ndarupplevelse
        await Task.Delay(2000);

        // TODO: Implementera verklig OpenAI Whisper API-integration
        // Exempel p√• hur det skulle se ut:
        /*
        var httpClient = _httpClientFactory.CreateClient("OpenAIClient");
        var apiKey = _configuration["OpenAI:ApiKey"];
        httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", apiKey);
        
        using var content = new MultipartFormDataContent();
        using var fileStream = audioFile.OpenReadStream();
        content.Add(new StreamContent(fileStream), "file", audioFile.FileName);
        content.Add(new StringContent("whisper-1"), "model");
        
        var response = await httpClient.PostAsync("https://api.openai.com/v1/audio/transcriptions", content);
        var result = await response.Content.ReadAsStringAsync();
        // ... hantera svar och skapa sammanfattning
        */

        // Testdata som simulerar realistiska AI-resultat
        return new TranscriptionResult
        {
            // Huvudtranskribering - simulerar vad AI skulle producera fr√•n ljudfilen
            Transcription = $"Detta √§r en testtranskribering av filen '{audioFile.FileName}'. " +
                          "M√∂tet behandlade projektets framsteg och viktiga beslut togs ang√•ende n√§sta fas. " +
                          "Deltagarna diskuterade utmaningar och m√∂jligheter som ligger framf√∂r teamet. " +
                          "Flera konkreta √•tg√§rder identifierades f√∂r att s√§kerst√§lla framg√•ng.",
            
            // AI-genererad sammanfattning av m√∂tets huvudpunkter
            Summary = "M√∂tet fokuserade p√• projektets status och planering av n√§sta steg. " +
                     "Teamet diskuterade framsteg, identifierade utmaningar och best√§mde prioriteringar. " +
                     "Flera konkreta √•tg√§rder beslutades f√∂r att driva projektet fram√•t.",
            
            // Lista av actionable items som AI extraherat fr√•n samtalet
            ActionPoints = new List<string>
            {
                "Slutf√∂ra designdokument till fredag",
                "Boka uppf√∂ljningsm√∂te n√§sta vecka", 
                "Kontakta externa leverant√∂rer f√∂r offerter",
                "Uppdatera projektplan baserat p√• nya krav",
                "F√∂rbereda presentation f√∂r styrelsen"
            }
        };
    }

    /// <summary>
    /// Sparar transkriberingsresultatet i databasen f√∂r framtida √•tkomst.
    /// 
    /// Denna metod skapar en permanent post av inspelningen och dess AI-analys
    /// s√• att anv√§ndare kan komma tillbaka och se historik av sina m√∂ten.
    /// 
    /// Databasen lagrar:
    /// - Koppling till den ursprungliga bokningen/m√∂tet
    /// - Anv√§ndarinformation f√∂r s√§kerhet och √•tkomstkontroll
    /// - Originalfilens metadata (namn, storlek)
    /// - AI-genererat inneh√•ll (transkribering, sammanfattning, √•tg√§rdspunkter)
    /// - Tidsst√§mplar f√∂r historik
    /// </summary>
    /// <param name="bookingId">ID f√∂r bokningen som inspelningen tillh√∂r</param>
    /// <param name="userId">Azure AD Object ID f√∂r anv√§ndaren</param>
    /// <param name="userName">Visningsnamn f√∂r anv√§ndaren</param>
    /// <param name="audioFile">Den ursprungliga ljudfilen f√∂r metadata</param>
    /// <param name="result">AI-transkriberingsresultatet</param>
    private async Task SaveTranscriptionResult(int bookingId, string userId, string userName, IFormFile audioFile, TranscriptionResult result)
    {
        try
        {
            // Skapa en ny databaspost med all relevant information
            var recording = new MeetingRecording
            {
                BookingId = bookingId,              // Koppla till ursprunglig bokning
                UserId = userId,                    // Azure AD Object ID f√∂r s√§kerhet
                UserName = userName,                // Visningsnamn f√∂r UI
                FileName = audioFile.FileName,      // Originalfilnamn f√∂r referens
                FileSizeBytes = audioFile.Length,   // Filstorlek f√∂r metadata
                DurationSeconds = 0,                // TODO: Implementera verklig ljudl√§ngdsanalys
                Transcription = result.Transcription, // Fullst√§ndig AI-transkribering
                Summary = result.Summary,           // AI-genererad sammanfattning
                KeyPoints = string.Join(";", result.ActionPoints), // √Ötg√§rdspunkter som semikolon-separerad str√§ng
                CreatedAt = DateTime.UtcNow,        // N√§r inspelningen skapades
                UpdatedAt = DateTime.UtcNow         // N√§r posten senast uppdaterades
            };

            // L√§gg till i databas och spara
            _context.MeetingRecordings.Add(recording);
            await _context.SaveChangesAsync();

            // Logga framg√•ng f√∂r fels√∂kning och audit trail
            _logger.LogInformation("Transkribering sparad f√∂r bokning {BookingId} av anv√§ndare {UserId}", bookingId, userId);
        }
        catch (Exception ex)
        {
            // Logga fel men l√•t det bubbla upp s√• upload-metoden kan hantera det
            _logger.LogError(ex, "Fel vid sparande av transkribering f√∂r bokning {BookingId}", bookingId);
            throw;
        }
    }

    /// <summary>
    /// K√∂r en faktisk OpenAI Whisper transkribering f√∂ljt av en chat completion f√∂r sammanfattning och √•tg√§rdspunkter.
    /// Denna metod anv√§nder named HttpClient "OpenAIClient" som konfigureras i Program.cs.
    /// </summary>
    private async Task<TranscriptionResult> TranscribeWithOpenAIAsync(IFormFile audioFile)
    {
        // Create client (Program.cs configures Authorization header if API key is present)
        var client = _httpClientFactory.CreateClient("OpenAIClient");

        // Log file details for debugging
        _logger.LogInformation("üéµ Sending to OpenAI Whisper: {FileName}, Size: {Size} bytes, ContentType: {ContentType}", 
            audioFile.FileName, audioFile.Length, audioFile.ContentType);

        byte[] audioData;
        string fileName = audioFile.FileName;
        using (var stream = audioFile.OpenReadStream())
        {
            audioData = new byte[audioFile.Length];
            var totalBytesRead = 0;
            while (totalBytesRead < audioData.Length)
            {
                var bytesRead = await stream.ReadAsync(audioData, totalBytesRead, audioData.Length - totalBytesRead);
                if (bytesRead == 0) break;
                totalBytesRead += bytesRead;
            }
        }

        // Retry with exponential backoff for transient errors (429/5xx)
        var maxRetries = 3;
        var attempt = 0;
        HttpResponseMessage? response = null;
        while (attempt <= maxRetries)
        {
            try
            {
                // Create fresh content for each attempt to avoid stream consumption issues
                using var content = new MultipartFormDataContent();
                var fileContent = new StreamContent(new MemoryStream(audioData));
                fileContent.Headers.ContentType = new System.Net.Http.Headers.MediaTypeHeaderValue("audio/mpeg");
                content.Add(fileContent, "file", fileName);
                content.Add(new StringContent("gpt-4o-mini-transcribe"), "model");
                
                // Use full URL to avoid NotFound error
                var whisperUrl = "https://api.openai.com/v1/audio/transcriptions";
                response = await client.PostAsync(whisperUrl, content);
                if (response.IsSuccessStatusCode)
                {
                    var respJson = await response.Content.ReadAsStringAsync();
                    // Whisper response structure varies; try to extract `text` field
                    using var doc = JsonDocument.Parse(respJson);
                    var root = doc.RootElement;
                    var transcriptionText = string.Empty;
                    if (root.TryGetProperty("text", out var textElem))
                    {
                        transcriptionText = textElem.GetString() ?? string.Empty;
                    }
                    else if (root.TryGetProperty("transcription", out var t2))
                    {
                        transcriptionText = t2.GetString() ?? string.Empty;
                    }

                    // Now use chat completions to produce summary and action points
                    var (summary, actions) = await PostToOpenAIChatAsync(transcriptionText);

                    return new TranscriptionResult
                    {
                        Transcription = transcriptionText,
                        Summary = summary,
                        ActionPoints = actions
                    };
                }

                // If client returned 429 or 5xx, consider retrying
                if ((int)response.StatusCode == 429 || (int)response.StatusCode >= 500)
                {
                    attempt++;
                    var delayMs = (int)Math.Pow(2, attempt) * 500;
                    await Task.Delay(delayMs);
                    continue;
                }

                // Non-retryable error -> throw so upper layer can handle/log
                var errorContent = await response.Content.ReadAsStringAsync();
                _logger.LogError("OpenAI Whisper returned error {Status}: {Content}", response.StatusCode, errorContent);
                
                // Try to parse OpenAI error for better messaging
                try
                {
                    using var errorDoc = JsonDocument.Parse(errorContent);
                    if (errorDoc.RootElement.TryGetProperty("error", out var errorObj))
                    {
                        var errorType = errorObj.TryGetProperty("type", out var typeElem) ? typeElem.GetString() : "unknown";
                        var errorMessage = errorObj.TryGetProperty("message", out var msgElem) ? msgElem.GetString() : response.StatusCode.ToString();
                        throw new Exception($"OpenAI Whisper error ({errorType}): {errorMessage}");
                    }
                }
                catch (JsonException)
                {
                    // If we can't parse the error, just use the raw response
                }
                
                throw new Exception($"OpenAI Whisper error: {response.StatusCode}");
            }
            catch (HttpRequestException ex)
            {
                attempt++;
                _logger.LogWarning(ex, "Transient error calling OpenAI Whisper (attempt {Attempt})", attempt);
                if (attempt > maxRetries) throw;
                await Task.Delay(200 * attempt);
            }
        }

        // If we get here, all retries failed
        throw new Exception("Failed to transcribe with OpenAI Whisper after retries");
    }

    /// <summary>
    /// Anropar OpenAI Chat Completions f√∂r att generera en kort summary och lista av action points.
    /// Returnerar tuple (summary, actions)
    /// </summary>
    private async Task<(string summary, List<string> actions)> PostToOpenAIChatAsync(string transcription)
    {
        var client = _httpClientFactory.CreateClient("OpenAIClient");

        // Bygg en koncis prompt f√∂r att extrahera summary och action items
    var systemMessage = "Du √§r en assistent som extraherar en kort, faktabaserad sammanfattning och en lista med konkreta √•tg√§rdspunkter (kortfattat) fr√•n ett m√∂tesprotokoll. Anv√§nd INTE emojis, symboler eller dekorativ text. √Öterge action points som en JSON-array med endast text.";
    var userMessage = $"Transkribering:\n\n{transcription}\n\nGe en kort sammanfattning (1-2 meningar, endast text, inga emojis) och en JSON-array med action points (endast text, inga emojis).";

        var requestObj = new
        {
            model = "gpt-4.1",
            messages = new[] {
                new { role = "system", content = systemMessage },
                new { role = "user", content = userMessage }
            },
            max_tokens = 300,
            temperature = 0.2
        };

        var json = JsonSerializer.Serialize(requestObj);
        using var content = new StringContent(json, System.Text.Encoding.UTF8, "application/json");

        var response = await client.PostAsync("/chat/completions", content);
        if (!response.IsSuccessStatusCode)
        {
            var err = await response.Content.ReadAsStringAsync();
            _logger.LogError("OpenAI Chat Completions error {Status}: {Content}", response.StatusCode, err);
            return ("", new List<string>());
        }

        var respJson = await response.Content.ReadAsStringAsync();
        using var doc = JsonDocument.Parse(respJson);
        var root = doc.RootElement;

        // Parse response - extract text from choices[0].message.content
        string assistantText = string.Empty;
        if (root.TryGetProperty("choices", out var choices) && choices.GetArrayLength() > 0)
        {
            var first = choices[0];
            if (first.TryGetProperty("message", out var message) && message.TryGetProperty("content", out var contentElem))
            {
                assistantText = contentElem.GetString() ?? string.Empty;
            }
        }

        // Separate summary and action points, but do NOT overwrite the transcript
        var summary = assistantText;
        var actions = new List<string>();

        // Find first occurrence of '[' for JSON array of action points
        var idx = assistantText.IndexOf('[');
        if (idx >= 0)
        {
            summary = assistantText.Substring(0, idx).Trim();
            var arrayText = assistantText.Substring(idx);
            try
            {
                var parsed = JsonSerializer.Deserialize<List<string>>(arrayText);
                if (parsed != null) actions = parsed;
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Could not parse action points JSON from assistant text");
            }
        }

        // Remove emojis from summary and action points
        summary = RemoveEmojis(summary);
        var cleanActions = actions.Select(RemoveEmojis).ToList();
        return (summary, cleanActions);

    }

    // Helper to remove emojis from a string
    private static string RemoveEmojis(string input)
    {
        if (string.IsNullOrEmpty(input)) return input;
        // Remove most common emoji unicode ranges
        return System.Text.RegularExpressions.Regex.Replace(input, "[\u2190-\u21FF\u2600-\u27BF\u1F300-\u1F6FF\u1F900-\u1F9FF\u1FA70-\u1FAFF\u200D\uFE0F]", "");
    }

    /// <summary>
    /// H√§mtar alla inspelningar f√∂r en specifik anv√§ndare.
    /// 
    /// Denna endpoint anv√§nds av profilvyn f√∂r att visa anv√§ndarens inspelningshistorik.
    /// Resultatet inkluderar metadata om inspelningen samt koppling till ursprungliga bokningen
    /// s√• att anv√§ndaren kan se vilket m√∂tesrum och vilken tid inspelningen gjordes.
    /// 
    /// S√§kerhet: Endast anv√§ndaren sj√§lv kan se sina egna inspelningar.
    /// Performance: Anv√§nder Entity Framework Include() f√∂r att ladda relaterade data i en query.
    /// </summary>
    /// <param name="userId">Azure AD Object ID f√∂r anv√§ndaren vars inspelningar ska h√§mtas</param>
    /// <returns>Lista av inspelningar med metadata och bokningsinformation</returns>
    [HttpGet("user/{userId}/recordings")]
    public async Task<IActionResult> GetUserRecordings(string userId)
    {
        try
        {
            // Kontrollera anv√§ndarauktorisering
            var currentUserId = HttpContext.User.Claims.FirstOrDefault(c => c.Type == "oid")?.Value;
            if (string.IsNullOrEmpty(currentUserId) || currentUserId != userId)
            {
                return Unauthorized("Inte beh√∂rig att se dessa inspelningar");
            }

            var recordings = await _context.MeetingRecordings
                .Include(mr => mr.Booking)
                .ThenInclude(b => b!.Resource)
                .Where(mr => mr.UserId == userId)
                .OrderByDescending(mr => mr.CreatedAt)
                .Select(mr => new
                {
                    id = mr.Id,
                    bookingId = mr.BookingId,
                    fileName = mr.FileName,
                    fileSizeBytes = mr.FileSizeBytes,
                    durationSeconds = mr.DurationSeconds,
                    transcription = mr.Transcription,
                    summary = mr.Summary,
                    keyPoints = mr.KeyPoints != null ? mr.KeyPoints.Split(';', StringSplitOptions.RemoveEmptyEntries) : new string[0],
                    createdAt = mr.CreatedAt,
                    booking = mr.Booking != null ? new
                    {
                        resourceName = mr.Booking.Resource!.Name,
                        startTime = mr.Booking.StartTime,
                        endTime = mr.Booking.EndTime
                    } : null
                })
                .ToListAsync();

            return Ok(recordings);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Fel vid h√§mtning av inspelningar f√∂r anv√§ndare {UserId}", userId);
            return StatusCode(500, "Fel vid h√§mtning av inspelningar");
        }
    }

    [HttpGet("meeting/{meetingId}/transcription")]
    public async Task<IActionResult> GetMeetingTranscription(string meetingId)
    {
        try
        {
            // Kontrollera anv√§ndarauktorisering
            var currentUserId = HttpContext.User.Claims.FirstOrDefault(c => c.Type == "oid")?.Value;
            if (string.IsNullOrEmpty(currentUserId))
            {
                return Unauthorized("Anv√§ndare inte identifierad");
            }

            if (!int.TryParse(meetingId, out int bookingId))
            {
                return BadRequest("Ogiltigt m√∂tes-ID");
            }

            var recording = await _context.MeetingRecordings
                .Include(mr => mr.Booking)
                .ThenInclude(b => b!.Resource)
                .FirstOrDefaultAsync(mr => mr.BookingId == bookingId && mr.UserId == currentUserId);

            if (recording == null)
            {
                return NotFound("Ingen transkribering hittades f√∂r detta m√∂te");
            }

            return Ok(new
            {
                id = recording.Id,
                meetingId = recording.BookingId,
                fileName = recording.FileName,
                transcription = recording.Transcription,
                summary = recording.Summary,
                keyPoints = recording.KeyPoints != null ? recording.KeyPoints.Split(';', StringSplitOptions.RemoveEmptyEntries) : new string[0],
                createdAt = recording.CreatedAt,
                booking = new
                {
                    resourceName = recording.Booking!.Resource!.Name,
                    startTime = recording.Booking.StartTime,
                    endTime = recording.Booking.EndTime
                }
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Fel vid h√§mtning av transkribering f√∂r m√∂te {MeetingId}", meetingId);
            return StatusCode(500, "Fel vid h√§mtning av transkribering");
        }
    }
}

public class TranscriptionResult
{
    public string Transcription { get; set; } = string.Empty;
    public string Summary { get; set; } = string.Empty;
    public List<string> ActionPoints { get; set; } = new();
}